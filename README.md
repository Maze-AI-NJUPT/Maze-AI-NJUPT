# Maze-AI-NJUPT
程序设计周

**（一）课题内容**

实现走迷宫。主要功能为界面显示、上下左右键的响应以及当前步数统计。通过该课题全面熟悉数组、字符串等的使用，掌握程序设计的基本方法及友好界面的设计。

**（二）课题要求**
1. 游戏界面显示：迷宫地图、上下左右移动的特效。
2. 动作选择：上下左右键对应于上下左右的移动功能，遇到障碍的处理。
3. 得分统计功能：步数等。
4. 设计一个自动走迷宫的程序，使得得到最短路径。
5. 实现至少三类算法：深度优先、广度优先、Q-learning、蚁群算法

**[Github项目地址](https://github.com/Maze-AI-NJUPT/Maze-AI-NJUPT/tree/%E5%88%86%E6%94%AF1)**

## 思路分析
项目采用 C++ 语言开发，以 Qt 作为游戏界面，而我负责迷宫地图以及寻路算法的实现。
### 迷宫的生成
主要涉及两个类：格子类以及迷宫类。
* 格子类用来保存迷宫地图中的类型，另外，还可以存储额外的信息，比如强化学习中的 Q 表和价值表等。
* 迷宫类则主要用来生成并保存迷宫的地图，并提供有关迷宫地图操作的一系列函数。诸如判断某一坐标是否可走、返回当前坐标下一个可走的方向、从当前坐标和方向得到下一步的坐标、获取坐标周围的墙体的坐标，以及更新某一坐标的价值等。

常用的迷宫生成算法有：随机prim、递归回溯、递归分割法。

#### 随机prim
随机prim 通过打通墙壁来生成迷宫。定义一个数组 ```blocks```，其中的元素为一个 ```pair```，保存一个墙体的坐标和方向，也就是从哪个方向到达这个墙体的。

算法流程如下：
1. 初始化地图中所有格子的类型为墙体。
2. 设置起点，并将起点的类型设置为道路。
3. 获取起点周围的墙体，加入到数组 ```blocks``` 中。
4. 当数组 ```blocks``` 不为空进入循环：
  * 从数组中随机取出一个墙体```(x,y)```
  * 从此墙体之前过来的方向，再前进一格，得到此坐标```(x1,y1)```
  * 判断前进到的坐标```(x1,y1)```是否为墙
  * 如果是，则将```(x,y)```和```(x1,y1)```的格子类型都置为道路，并获取```(x1,y1)```周围的墙体，加入到数组 ```blocks``` 中。
  * 从数组中删除墙体```(x,y)```
5. 设置终点坐标，默认为 ```(row-2,col-2)```

### 迷宫寻路算法
比较常见的诸如```DFS```、```BFS```就不再介绍了，这里讲讲要求里的用强化学习来寻找路径。

在强化学习中，有几个关键的概念，分别是环境、智能体、动作、状态。这是类比人类学习的过程，其中智能体通过观察环境得知对应的状态，然后进行相应的动作，接着环境会根据智能体的动作给出相应的奖励，如果智能体做对了，就给出一个正的奖励，否则给出一个负的奖励，智能体通过观察得知奖励的变化，计算下一步动作的价值，从而选择价值最高的动作。所以环境会有一个奖励表，分别记录着当前状态和下一步动作的奖励，这个奖励表是固定的。而智能体则维护着一张Q表，记录当前状态和下一步动作的价值。而智能体的训练过程则是不断地从当前状态计算下一步的价值，更新价值到Q表，然后跳转到下一个状态。刚开始的时候，智能体可能会尝试错误的状态，而当训练次数不断增多，就会稳定到一个固定的状态路径，即得到一个解。而在具体实现中，我并没有明显地定义这两张表，而是将价值和奖励都记录在了迷宫地图的格子中。

#### QLearning
QLearning 的核心是 Q表，其中迷宫地图中的每一个格子对应于一个状态。训练过程就是智能体不断试错的过程，智能体不断地更新 Q表中每个状态和下一步动作，选择价值最大化的动作。

通常的训练过程：智能体从初始状态出发，选择价值最大化的动作并更新状态（为了智能体有一定的探索性，以一定的概率不选择价值最大化的动作而是随机选择一个动作），然后做出已选择的动作到达下一状态，周而复始，直到到达结束状态，这样一次训练就结束了，然后又重新开始训练，直到达到预定的训练次数或者已经求出解。

但是，在我们这个项目的迷宫地图中，没有陷阱和宝箱之类的，只有道路和墙，而且智能体在训练的过程通过判断选择不会走墙体的动作，因此在智能体看来，大部分都是价值为 0 的道路，而只有一个终点是价值为 1。这样设置奖励表存在一个问题：就是智能体在没有探索到终点之前，其通过公式更新的价值总是为 0，这显然是没有意义的，只会徒增训练次数。于是可以这样设置奖励表，将道路的奖励设置为 -1，而终点的奖励设置为 0，这样可以保证每次更新到的价值都是有意义的。

QLearning的算法流程伪代码：
![QLearning](http://ww1.sinaimg.cn/large/005JD0Ejgy1ggd24uf8hcj30ir07qmzh.jpg)

#### Q(Lambda)
Q(Lambda) 是在QLearning的基础上的修改版。Q(lambda) 使用了一种叫资格迹的方法，它主要来自于解释奖励的由来。

改进之后的算法流程与原先的区别：

每次开始之前所有资格迹置为0，然后每转移到一个状态，当前位置的资格迹就加1，然后价值的公式的后半部分就乘上资格迹，然后选择动作，如果是随机探索，那么就将资格迹置为0，而如果是最大化价值动作，则将资格迹乘上折扣因子与lambda的乘积进行衰减。那么这种训练方法就使用尝试修改的第一种训练方法，遍历迷宫从一个状态出发转移到下一状态，周而复始直到路径被找到。

但是我实际测试发现这种方法还是不如直接遍历迷宫更新Q表中的每一个格子的价值来的快。

在测试中我们发现随机探索的概率越高，使用QLearning和QLambda的训练次数的差异就会越大，而两者相比更低的随机探索概率而言其训练次数也会增多。

Q(Lambda)的算法流程伪代码：
![QLambda](https://tvax2.sinaimg.cn/large/005JD0Ejgy1gge6nkdiycj30vg0mzn0e.jpg)